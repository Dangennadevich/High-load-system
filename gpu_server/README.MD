<h1 align="center">Проект по распознованию сгенерированного AI моделью текста</h1>

Проект по распознованию AI-сгенерированного текста, основные компоненты:

    * CPU сервер с FastAPI сервером, на котором будет происходить взаимодействие с пользователем.
    
    * GPU сервер с AI моделью, которая проводит скоринг по запросу с CPU сервера.


<h2 align="center">GPU часть проекта</h2>

Запускается на втором сервере архитектуры - GPU. Сервис ожидает сообщение пользователя от CPU сервера, выполняет обработку, результат записывает в БД.

Клиент отправляет запрос на CPU сервер (бекенд веб приложения), CPU сервер отправляет запрос на обработку текста в GPU сервер через брокера сообщений rabbitmq и записывает лог в БД PSQL. GPU получает запрос на обработку через rabbitmq, результат кладет в БД обновляя лог оставленный CPU сервисом.
 

Связи сервиса с другими объектами архитектуры: **CPU >> GPU >> PSQL DB**


<h2 align="center">Настройка сервисов на GPU</h2>

Выполняется после развертывания необходимого ПО на сервере (~/README.MD, пункт 1-3) и поднятии инфраструктуры на CPU сервере (~/cpu_server/README.MD)

<h3 align="center">2. Поднимаем сервисы </h3>

<b>2.1 Создаем docker образ (Только из Dockerfile)</b>

<code>docker build -t gpu .</code> 

<b>2.2 Запускаем сервис  </b>

<code>docker run gpu </code> 


<h1 align="center">Файлы из директории GPU сервера</h1>

**main.py** - скрипт для запуска сервиса на GPU сервере. В дальнейшем тут будет работать AI модель по распознованию
сгенерированного текста другой AI моделью, на данный момент тут стоит заглушка np.random.rand()

**Dockerfile** Инструкция для развертывания сервиса с "моделью"

**pyproject.toml** Poetry инструкция для настройки виртуального окружения